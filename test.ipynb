{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dcc2658-55fc-426b-9cae-2d8fec2bbe4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start import\n",
      "end import\n",
      "Using device: cuda\n",
      "Loading model checkpoint...\n",
      "Found 762 images\n",
      "\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:18<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len pred 762\n",
      "Len target 762\n",
      "\n",
      "Evaluation Results:\n",
      "Mean IoU: 0.7033\n",
      "mAP @ 0.5: 0.9525\n",
      "Number of validation samples: 762\n",
      "\n",
      "Saving predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:17<00:00,  5.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions saved to data/output/labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"start import\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchmetrics.detection import IntersectionOverUnion\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "print(\"end import\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['NO_ALBUMENTATIONS_UPDATE'] = '1'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "class DigitDataset:\n",
    "    def __init__(self, image_dir, label_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.images = [f for f in sorted(os.listdir(image_dir)) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"Found {len(self.images)} images\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image_path = os.path.join(self.image_dir, self.images[idx])\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            # Apply CLAHE preprocessing\n",
    "            lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "            l, a, b = cv2.split(lab)\n",
    "            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "            cl = clahe.apply(l)\n",
    "            enhanced = cv2.merge((cl,a,b))\n",
    "            image = cv2.cvtColor(enhanced, cv2.COLOR_LAB2RGB)\n",
    "            \n",
    "            label_path = os.path.join(self.label_dir, self.images[idx].replace('.jpg', '.txt'))\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            \n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    class_id, x, y, w, h = map(float, line.strip().split())\n",
    "                    adjusted_class_id = int(class_id) + 1\n",
    "                    \n",
    "                    x1 = max(0, (x - w/2) * image.shape[1])\n",
    "                    y1 = max(0, (y - h/2) * image.shape[0])\n",
    "                    x2 = min(image.shape[1], (x + w/2) * image.shape[1])\n",
    "                    y2 = min(image.shape[0], (y + h/2) * image.shape[0])\n",
    "                    \n",
    "                    if x2 <= x1 or y2 <= y1 or (x2 - x1) < 6 or (y2 - y1) < 6:\n",
    "                        continue\n",
    "                    \n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(adjusted_class_id)\n",
    "            \n",
    "            if not boxes:\n",
    "                return self.__getitem__((idx + 1) % len(self))\n",
    "            \n",
    "            boxes = np.array(boxes, dtype=np.float32)\n",
    "            labels = np.array(labels, dtype=np.int64)\n",
    "            \n",
    "            if self.transforms:\n",
    "                transformed = self.transforms(\n",
    "                    image=image,\n",
    "                    bboxes=boxes,\n",
    "                    class_labels=labels\n",
    "                )\n",
    "                image = transformed['image']\n",
    "                boxes = transformed['bboxes']\n",
    "                labels = transformed['class_labels']\n",
    "            \n",
    "            target = {\n",
    "                'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
    "                'labels': torch.as_tensor(labels, dtype=torch.int64)\n",
    "            }\n",
    "            \n",
    "            return image, target\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {self.images[idx]}: {str(e)}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "def get_transform():\n",
    "    return A.Compose([\n",
    "        A.Resize(640, 640, always_apply=True),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(\n",
    "        format='pascal_voc',\n",
    "        label_fields=['class_labels'],\n",
    "        min_visibility=0.6\n",
    "    ))\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, device, confidence_threshold=0.5):\n",
    "    model.eval()\n",
    "    IOU_metric = IntersectionOverUnion(class_metrics=True)\n",
    "    MAP_metric = MeanAveragePrecision(iou_type=\"bbox\", box_format =\"xyxy\")\n",
    "    \n",
    "    preds_list = []\n",
    "    target_list = []\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            outputs = model(images)\n",
    "            #print(\"out: \", len(outputs))\n",
    "            #print(\"targ: \", len(targets))\n",
    "            for pred, target in zip(outputs, targets):\n",
    "                \n",
    "                \n",
    "                pred_boxes = pred['boxes'].cpu()\n",
    "                pred_labels = pred['labels'].cpu()\n",
    "                pred_scores = pred['scores'].cpu()\n",
    "                \n",
    "                # Filter predictions by confidence threshold\n",
    "                mask = pred_scores > confidence_threshold\n",
    "                pred_boxes = pred_boxes[mask]\n",
    "                pred_labels = pred_labels[mask]\n",
    "                pred_scores = pred_scores[mask]\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                pred_dict = {\n",
    "                    'boxes': pred_boxes,\n",
    "                    'labels': pred_labels,\n",
    "                    'scores': pred_scores\n",
    "                }\n",
    "                \n",
    "                target_boxes = target['boxes'].cpu()\n",
    "                target_labels = target['labels'].cpu()\n",
    "                \n",
    "                target_dict = {\n",
    "                    'boxes': target_boxes,\n",
    "                    'labels': target_labels\n",
    "                }\n",
    "                #print()\n",
    "                #print(\"predL: \", pred_labels)\n",
    "                #print(\"targL: \", target_labels)\n",
    "                #print(\"predB: \", pred_boxes)\n",
    "                #print(\"targB: \", target_boxes)\n",
    "                #print()\n",
    "                \n",
    "                preds_list.append(pred_dict)\n",
    "                target_list.append(target_dict)\n",
    "            \n",
    "            \n",
    "    \n",
    "    print(\"Len pred\", len(preds_list))\n",
    "    print(\"Len target\", len(target_list))\n",
    "    IOU_metric(preds_list, target_list)\n",
    "    MAP_metric(preds_list, target_list)\n",
    "    \n",
    "    \n",
    "    return IOU_metric, MAP_metric\n",
    "\n",
    "def save_predictions_to_txt(model, data_loader, device, output_dir, confidence_threshold=0.5):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"\\nSaving predictions...\")\n",
    "    \n",
    "    current_idx = 0  # Keep track of global image index\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader):\n",
    "            images = [image.to(device) for image in images]\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for pred in outputs:\n",
    "                # Get image filename using global index\n",
    "                image_filename = data_loader.dataset.images[current_idx]\n",
    "                output_filename = os.path.join(output_dir, os.path.splitext(image_filename)[0] + '.txt')\n",
    "                \n",
    "                # Filter predictions by confidence\n",
    "                pred_boxes = pred['boxes'].cpu().numpy()\n",
    "                pred_scores = pred['scores'].cpu().numpy()\n",
    "                pred_labels = pred['labels'].cpu().numpy()\n",
    "                \n",
    "                mask = pred_scores > confidence_threshold\n",
    "                pred_boxes = pred_boxes[mask]\n",
    "                pred_labels = pred_labels[mask]\n",
    "                \n",
    "                # Save predictions in YOLO format\n",
    "                with open(output_filename, 'w') as f:\n",
    "                    for box, label in zip(pred_boxes, pred_labels):\n",
    "                        # Convert Pascal VOC format to YOLO format\n",
    "                        x1, y1, x2, y2 = box\n",
    "                        width = x2 - x1\n",
    "                        height = y2 - y1\n",
    "                        x_center = x1 + width / 2\n",
    "                        y_center = y1 + height / 2\n",
    "                        \n",
    "                        # Normalize coordinates by image size (640x640)\n",
    "                        x_center /= 640\n",
    "                        y_center /= 640\n",
    "                        width /= 640\n",
    "                        height /= 640\n",
    "                        f.write(f\"{label-1} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "                \n",
    "                current_idx += 1  # Increment global index after each image\n",
    "    \n",
    "    print(f\"\\nPredictions saved to {output_dir}\")\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = fasterrcnn_resnet50_fpn_v2(\n",
    "        weights='DEFAULT',\n",
    "        box_score_thresh=0.01,\n",
    "        box_nms_thresh=0.45,\n",
    "        box_detections_per_img=3,\n",
    "        rpn_pre_nms_top_n_train=2000,\n",
    "        rpn_post_nms_top_n_train=1000,\n",
    "        rpn_pre_nms_top_n_test=1000,\n",
    "        rpn_post_nms_top_n_test=500,\n",
    "        rpn_score_thresh=0.01,\n",
    "        rpn_nms_thresh=0.7,\n",
    "        min_size=1024,\n",
    "        max_size=1600\n",
    "    )\n",
    "    \n",
    "    num_classes = 11\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    print(\"Loading model checkpoint...\")\n",
    "    checkpoint = torch.load('best_model.pth', map_location=device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    val_dataset = DigitDataset(\n",
    "        image_dir='data/val/images',\n",
    "        label_dir='data/val/labels',\n",
    "        transforms=get_transform()\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        collate_fn=lambda x: tuple(zip(*x)),\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    IOU_metric, MAP_metric = evaluate_model(model, val_loader, device)\n",
    "    mean_iou = IOU_metric.compute()['iou'].item()\n",
    "    mAP = MAP_metric.compute()['map_50'].item()\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f'Mean IoU: {mean_iou:.4f}')\n",
    "    print(f'mAP @ 0.5: {mAP:.4f}')\n",
    "    print(f'Number of validation samples: {len(val_dataset)}')\n",
    "    \n",
    "    save_predictions_to_txt(model, val_loader, device, 'data/output/labels')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c10113-5adb-482a-91e6-f5301f876e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testYML",
   "language": "python",
   "name": "testyml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
